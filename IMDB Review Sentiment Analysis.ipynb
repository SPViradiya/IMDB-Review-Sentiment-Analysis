{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of class\n",
    "\n",
    "I've created one class named \"predictive_analysis\" which has different methods. \n",
    "It is flexibal so that user just need to call it once and it will store training and testing dataset in internal variable and user do not need to pass it every time.\n",
    "\n",
    "Here are the list of methods with exaplanation:\n",
    "\n",
    "<ol>\n",
    "    <li><b>load_data:</b> This method will ask user to give training and testing file path. It will store the data internally</li>\n",
    "    <li><b>transform_tfidf:</b> This will normalize training and testing datset using sklean TfidfTransformer</li>\n",
    "    <li><b>convert_to_binary:</b> To convert rate into binary</li>\n",
    "    <li><b>logistic_regression:</b> To perform Logistic Regression with option for cross validation</li>\n",
    "    <li><b>linear_svc:</b> To perform Lieanr SVC with option for cross validation</li>\n",
    "    <li><b>decision_tree:</b> To perform Decision Tree with option for cross validation</li>\n",
    "    <li><b>random_forest:</b> To perform Random Forest with option for cross validation</li>\n",
    "    <li><b>prediction_save_csv:</b> To predict output from given object of method and save it into CSV file</li>\n",
    "</ol>\n",
    "\n",
    "## Please note that when you call load_data() function, you need to give input before running any other cell. If you run any other cell without giving it input, Jupyter get stuck as it runs one kernal at a time. It will wait for the input and stuck there while you were trying to run other code and you will need to shutdown the file before running it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required libraries and modules\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from time import time \n",
    "import re\n",
    "\n",
    "\n",
    "class predictive_models:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.training_data = \"\"\n",
    "        self.testing_data = \"\"\n",
    "        self.training_target = \"\"\n",
    "        self.testing_target = \"\"\n",
    "        \n",
    "    #def load_data(self, training_data_path, testing_data_path):\n",
    "    def load_data(self):\n",
    "        training_data_path = input(\"Path for training dataset\")        \n",
    "        \n",
    "        if training_data_path.strip() == \"\" or re.search(r\".feat\",training_data_path) is None:\n",
    "            print(\"Please enter valid path for training dataset\")\n",
    "        else:\n",
    "            testing_data_path = input(\"Path for testing dataset\")\n",
    "            \n",
    "            if testing_data_path.strip() == \"\" or re.search(r\".feat\",testing_data_path) is None: \n",
    "                print(\"Please enter valid path for testing dataset\")\n",
    "            else:\n",
    "                self.training_data, self.training_target = load_svmlight_file(training_data_path)\n",
    "\n",
    "                # Please note that number of features needs to be same in training and testing dataset\n",
    "                # Here training dataset has 89527 features where as testing dataset has 89523 features only\n",
    "                # We specifically need to specify n_features for testing dataset\n",
    "                self.testing_data, self.testing_target = load_svmlight_file(testing_data_path, n_features=89527)\n",
    "            \n",
    "        \n",
    "    #\n",
    "    # Preprocessing and normalization\n",
    "    #\n",
    "\n",
    "    \"\"\"\n",
    "    Transform data into TF-IDF for the normalization\n",
    "\n",
    "    Parameters:\n",
    "        trainig_data (list | matrix | dictionaty): Dataset which needs to be transformed\n",
    "        testing_data (list | matrix | dictionaty): Dataset which needs to be transformed\n",
    "\n",
    "    Returns:\n",
    "        (list | matrix | dictionaty): Transformed data as same format as given when calling the function\n",
    "    \"\"\"\n",
    "    def transform_tfidf(self):\n",
    "        if self.training_data == \"\" or self.testing_data == \"\":\n",
    "            print(\"Please call load_data() method first and give path for training and testing dataset\")\n",
    "            return\n",
    "            \n",
    "        tfidf_transform = TfidfTransformer()\n",
    "\n",
    "        # We are using .fit_transform for training dataset as we want to let our system count TF for every single review\n",
    "        # and IDF from each review once and that will be from trainng dataset only\n",
    "        # We want to just transform the test dataset into TF-IDF based on IF from each review from testing dataset but\n",
    "        # TF-IDF will use IDF from TRAINING DATASET.\n",
    "\n",
    "        self.training_data = tfidf_transform.fit_transform(self.training_data)\n",
    "        self.testing_data = tfidf_transform.transform(self.testing_data) # Only transform as it will use IDF of training dataset\n",
    "        \n",
    "    #\n",
    "    # Convert target into binary. if > 5 than 1, else 0 ( Preprocessing )\n",
    "    #\n",
    "    def convert_to_binary(self, target_data):\n",
    "        if self.training_data == \"\" or self.testing_data == \"\":\n",
    "            print(\"Please call load_data() method first and give path for training and testing dataset\")\n",
    "            return\n",
    "        \n",
    "        target = []\n",
    "        for i in range(len(target_data)):\n",
    "            if target_data[i] > 5:\n",
    "                target.append(1) # Positive review\n",
    "            else:\n",
    "                target.append(0) # Negative review\n",
    "        return target\n",
    "\n",
    "    def convert_target_labels(self):\n",
    "        if self.training_data == \"\" or self.testing_data == \"\":\n",
    "            print(\"Please call load_data() method first and give path for training and testing dataset\")\n",
    "            return\n",
    "        \n",
    "        # Please note that the same variable now the target will have binary values\n",
    "        self.training_target = self.convert_to_binary(self.training_target)\n",
    "        self.testing_target = self.convert_to_binary(self.testing_target)\n",
    "        \n",
    "    def logistic_regression(self, cros_valid = False):\n",
    "        if self.training_data == \"\" or self.testing_data == \"\":\n",
    "            print(\"Please call load_data() method first and give path for training and testing dataset\")\n",
    "            return\n",
    "        \n",
    "        start = time()\n",
    "        log_reg = LogisticRegression(max_iter = 1000) # Initializing\n",
    "\n",
    "        if cros_valid == False:\n",
    "            print(\"Logistic Regression without cross validation\")\n",
    "            \n",
    "            log_reg.fit(self.training_data, self.training_target) # Training\n",
    "\n",
    "            # Check with testing dataset\n",
    "            print(\"Accuracy with testing dataset is\",log_reg.score(self.testing_data, self.testing_target) * 100,\"%\")\n",
    "\n",
    "        else:\n",
    "            print(\"\\n Logistic Regression with 10-fold\")\n",
    "            # Cross validation\n",
    "            log_reg_score = cross_val_score(log_reg, self.training_data, self.training_target, cv = 10)\n",
    "            print(log_reg_score)\n",
    "            print(\"Mean\",log_reg_score.mean(),\" \",\"Min:\",log_reg_score.min(),\" \",\"Max: \",log_reg_score.max())\n",
    "\n",
    "\n",
    "        end = time()\n",
    "        print(\"Logistic regression ran in \",str(round((end-start), 2)),\"seconds\")\n",
    "        return\n",
    "    \n",
    "    def linear_svc(self, cros_valid = False):\n",
    "        if self.training_data == \"\" or self.testing_data == \"\":\n",
    "            print(\"Please call load_data() method first and give path for training and testing dataset\")\n",
    "            return\n",
    "        \n",
    "        start = time()\n",
    "        if cros_valid == False:\n",
    "            print(\"Linear SVC\")\n",
    "            l_svc = LinearSVC() # Initializing\n",
    "            l_svc.fit(self.training_data, self.training_target) # Training\n",
    "\n",
    "            # Check with testing dataset\n",
    "            print(\"Accuracy with testing dataset is\",l_svc.score(self.testing_data, self.testing_target) * 100,\"%\")\n",
    "        else:\n",
    "            print(\"\\nFinding Best C for Linear SVC with 3-fold\")\n",
    "\n",
    "            # I've just passed 6 C values here as this is taking too much memory and time to run\n",
    "            # Demonstrating the possibilities and how to get best C value\n",
    "            c_list = 2**np.array(range(-2, 6), dtype='float')\n",
    "            cv_scores = []\n",
    "            for c in c_list:\n",
    "                l_svc= LinearSVC(C=c, dual=False)\n",
    "                score = cross_val_score(l_svc, self.training_data, self.training_target, cv=3)\n",
    "                cv_scores.append(score.mean()*100)\n",
    "                bestscore, bestC = max([(val, c_list[idx]) for (idx, val) in enumerate(cv_scores)])\n",
    "            print('Best CV accuracy =', round(bestscore,2), '% achieved at C =', bestC)\n",
    "\n",
    "            print(\"\\nUsing C =\", bestC, \"to train the data again test on testing data\")\n",
    "\n",
    "            # retrain on whole trainning set using best C value obtained from Cross validation\n",
    "            l_svc = LinearSVC(C=bestC)\n",
    "            l_svc.fit(self.training_data, self.training_target)\n",
    "            accu = l_svc.score(self.testing_data, self.testing_target)*100\n",
    "            print('Test accuracy =', accu, 'achieved at C =', bestC)\n",
    "\n",
    "        end = time()\n",
    "        print(\"Linear SVC ran in \",str(round((end-start), 2)),\"seconds\")\n",
    "        return\n",
    "    \n",
    "    def decision_tree(self, cros_valid = False):\n",
    "        if self.training_data == \"\" or self.testing_data == \"\":\n",
    "            print(\"Please call load_data() method first and give path for training and testing dataset\")\n",
    "            return\n",
    "        \n",
    "        start = time()\n",
    "        if cros_valid == False:\n",
    "            print('Training decision tree with depth 1')\n",
    "            dec_tree = DecisionTreeClassifier(max_features='auto', max_depth=1)\n",
    "            dec_tree.fit(self.training_data, self.training_target)\n",
    "            print('Accuracy = %.2f%%'  % (dec_tree.score(self.testing_data, self.testing_target)*100))\n",
    "        else:\n",
    "            print(\"\\nFinding max depth for decision tree\")\n",
    "            parameters = {'max_depth':range(3,20)}\n",
    "            clf = GridSearchCV(DecisionTreeClassifier(), parameters, n_jobs=4)\n",
    "            clf.fit(self.training_data, self.training_target)\n",
    "            tree_model = clf.best_estimator_\n",
    "            print('Accuracy = %.2f%%' % (clf.best_score_ * 100),\" with \",clf.best_params_)\n",
    "\n",
    "            print(\"\\nUsing \",clf.best_params_,\" to train the data again and test on testing data\")\n",
    "            dec_tree = DecisionTreeClassifier(max_depth = clf.best_params_['max_depth'])\n",
    "            dec_tree.fit(self.training_data, self.training_target)\n",
    "            print('Accuracy = %.2f%%'  % (dec_tree.score(self.testing_data, self.testing_target)*100))\n",
    "\n",
    "\n",
    "        end = time()\n",
    "        print(\"Decision Tree ran in \",str(round((end-start), 2)),\"seconds\")\n",
    "        return\n",
    "    \n",
    "    def random_forest(self, cros_valid = False):\n",
    "        if self.training_data == \"\" or self.testing_data == \"\":\n",
    "            print(\"Please call load_data() method first and give path for training and testing dataset\")\n",
    "            return\n",
    "        \n",
    "        start = time()\n",
    "\n",
    "        if cros_valid == False:\n",
    "            print(\"Training Random Forest with 100 trees with depth 16\")\n",
    "            clf_forest = RandomForestClassifier(n_estimators = 100, min_samples_leaf=5, max_depth=16)\n",
    "            clf_forest.fit(self.training_data, self.training_target)\n",
    "            print('Accuracy = %.2f%%' % (clf_forest.score(self.testing_data, self.testing_target)*100))\n",
    "        else:\n",
    "            print(\"\\nFinding best parameters for Random Forest\")\n",
    "            param_grid = { \n",
    "                'n_estimators': [100, 300, 500, 700, 1200],\n",
    "                'max_depth': [5, 8, 15, 25, 30],\n",
    "                'min_samples_split' : [2, 5, 10, 15, 100],\n",
    "                'min_samples_leaf': [1, 2, 5, 10]\n",
    "            }\n",
    "\n",
    "            cv_rfc = GridSearchCV(estimator= RandomForestClassifier(), param_grid=param_grid, cv= 3)\n",
    "            cv_rfc.fit(self.training_data, self.training_target)\n",
    "            print('Accuracy = %.2f%%' % (cv_rfc.best_score_ * 100),\" with \",cv_rfc.best_params_)\n",
    "            \n",
    "            print(\"\\nUsing \",cv_rfc.best_params_,\" to train the data again and test on testing data\")\n",
    "            clf_forest = RandomForestClassifier(n_estimators = cv_rfc.best_params_['n_estimators'], min_samples_leaf=cv_rfc.best_params_['min_samples_leaf'], max_depth=cv_rfc.best_params_['max_depth'],min_samples_split=cv_rfc.best_params_['min_samples_split'])\n",
    "            clf_forest.fit(self.training_data, self.training_target)\n",
    "            print('Accuracy = %.2f%%' % (clf_forest.score(self.testing_data, self.testing_target)*100))\n",
    "\n",
    "        end = time()\n",
    "        print(\"Random Forest ran in \",str(round((end-start), 2)),\"seconds\")\n",
    "        return\n",
    "    \n",
    "    # Predicting the data from random forest for the time being \n",
    "    def prediction_save_csv(self, clf_object,file_name):\n",
    "        if self.training_data == \"\" or self.testing_data == \"\":\n",
    "            print(\"Please call load_data() method first and give path for training and testing dataset\")\n",
    "            return\n",
    "        \n",
    "        clf_object.fit(self.training_data, self.training_target)\n",
    "        pred_data = clf_object.predict(self.testing_data)\n",
    "        print((clf_object.score(self.testing_data, self.testing_target)*100))\n",
    "        prediction = []\n",
    "        for i in range(len(pred_data)):\n",
    "            if pred_data[i] == 0:\n",
    "                prediction.append(str(i) + \", negative\") \n",
    "            else:\n",
    "                prediction.append(str(i) + \", positive\")\n",
    "                \n",
    "        print(\"Creating and writing CSV\")\n",
    "        csv_file = open(file_name,'w')\n",
    "        csv_file.write(\"\\n\".join(prediction))\n",
    "        csv_file.close()\n",
    "        \n",
    "        print(\"File creation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path for training dataset\n",
      "Please enter valid path for training dataset\n"
     ]
    }
   ],
   "source": [
    "# Loading and prepating the data\n",
    "pred_models = predictive_models()\n",
    "\n",
    "#training_data_path = \"./aclImdb/train/labeledBow.feat\"\n",
    "#testing_data_path = \"./aclImdb/test/labeledBow.feat\"\n",
    "\n",
    "# Loading the data\n",
    "#pred_models.load_data(training_data_path,testing_data_path)\n",
    "pred_models.load_data() # This will ask user to give absolute path of training and testing dataset and validate the file type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please call load_data() method first and give path for training and testing dataset\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing and normalisation\n",
    "pred_models.transform_tfidf()\n",
    "\n",
    "# Convert target data to binary based on >5 value\n",
    "pred_models.convert_target_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with testing dataset is 88.31599999999999 %\n",
      "Logistic regression ran in  1.63 seconds\n",
      "Cross validation with 10-fold\n",
      "[0.8676 0.8556 0.8692 0.8716 0.8492 0.8796 0.8756 0.8724 0.8792 0.8616]\n",
      "Mean 0.8681599999999999   Min: 0.8492   Max:  0.8796\n",
      "Logistic regression ran in  14.03 seconds\n"
     ]
    }
   ],
   "source": [
    "# Logisic Regression\n",
    "pred_models.logistic_regression()\n",
    "\n",
    "#10-fold\n",
    "pred_models.logistic_regression(cros_valid= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC\n",
      "Accuracy with testing dataset is 87.896 %\n",
      "Linear SVC ran in  0.47 seconds\n",
      "\n",
      "Finding Best C for Linear SVC with 3-fold\n",
      "Best CV accuracy = 86.32 % achieved at C = 0.25\n",
      "\n",
      "Checking with test dataset with C = 0.25\n",
      "Test accuracy = 88.628 achieved at C = 0.25\n",
      "Linear SVC ran in  29.0 seconds\n"
     ]
    }
   ],
   "source": [
    "# Linear SVC\n",
    "pred_models.linear_svc()\n",
    "\n",
    "# Finding best C with 3-fold\n",
    "pred_models.linear_svc(cros_valid= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training decision tree with depth 1\n",
      "Accuracy = 50.44%\n",
      "Decision Tree ran in  0.15 seconds\n",
      "\n",
      "Finding max depth for decision tree\n",
      "Accuracy = 72.60%  with  {'max_depth': 16}\n",
      "\n",
      "Using  {'max_depth': 16}  to train the data again and test on testing data\n",
      "Accuracy = 72.68%\n",
      "Decision Tree ran in  171.82 seconds\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "pred_models.decision_tree()\n",
    "\n",
    "# Checking max depth\n",
    "pred_models.decision_tree(cros_valid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest with 100 tress with depth 16\n",
      "Accuracy = 82.83%\n",
      "Random Forest ran in  3.35 seconds\n",
      "\n",
      "Finding best parameters for Random Forest\n",
      "Accuracy = 83.88% with {'max_depth': 15, 'min_samples_leaf': 5, 'min_samples_split': 2,'n_estimators': 700}\n",
      "\n",
      "Using  {'max_depth': 15, 'min_samples_leaf': 5, 'min_samples_split': 2,'n_estimators': 700}  to train the data again and test on testing data\n",
      "Accuracy = 83.56%\n",
      "Random Forest ran in  4231.20 seconds\n"
     ]
    }
   ],
   "source": [
    "# Random forest tree\n",
    "pred_models.random_forest()\n",
    "\n",
    "pred_models.random_forest(cros_valid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.52000000000001\n",
      "Creating and writing CSV\n",
      "File creation completed\n"
     ]
    }
   ],
   "source": [
    "# Store predicted data into CSV file using Random Forest\n",
    "clf_random_forest = RandomForestClassifier(max_depth = 15, n_estimators = 700, min_samples_split = 2, min_samples_leaf = 5)\n",
    "\n",
    "pred_models.prediction_save_csv(clf_random_forest, \"prediction_random_forst.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.31599999999999\n",
      "Creating and writing CSV\n",
      "File creation completed\n"
     ]
    }
   ],
   "source": [
    "# Store predicted data into CSV file using Logistic Regresstion\n",
    "clf_lreg = LogisticRegression(max_iter = 1000)\n",
    "\n",
    "pred_models.prediction_save_csv(clf_lreg, \"prediction_logistic_regression.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
